#!/usr/bin/env python
"""
é‡æ„ç‰ˆé‡å­vså¤å…¸ç”Ÿæˆå™¨é€epochæ€§èƒ½å¯¹æ¯”åˆ†æè„šæœ¬
å®ç°é€epochçš„å®Œæ•´æŒ‡æ ‡å†å²è¿½è¸ªå’Œå¯è§†åŒ–
"""

import mlflow
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import warnings
from pathlib import Path
import sys

warnings.filterwarnings('ignore')

# å…¨å±€å˜é‡æ§åˆ¶æ ‡ç­¾è¯­è¨€
use_english_labels = True  # å¼ºåˆ¶ä½¿ç”¨è‹±æ–‡æ ‡ç­¾é¿å…å­—ä½“é—®é¢˜

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
def setup_chinese_fonts():
    """è®¾ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ"""
    try:
        # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.family'] = ['DejaVu Sans']
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # éªŒè¯å­—ä½“è®¾ç½®
        fig, ax = plt.subplots(figsize=(1, 1))
        ax.text(0.5, 0.5, 'æµ‹è¯•', fontsize=12)
        plt.close(fig)
        
    except Exception as e:
        print(f"ä¸­æ–‡å­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾: {e}")
        # å¦‚æœä¸­æ–‡ä¸å¯ç”¨ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
        global use_english_labels
        use_english_labels = True
    
    # è®¾ç½®seabornæ ·å¼
    sns.set_style("whitegrid")
    sns.set_palette("husl")

def get_run_metric_history(client: mlflow.tracking.MlflowClient, 
                          run_id: str, 
                          metric_name: str) -> List[Tuple[int, float]]:
    """è·å–æŒ‡å®šè¿è¡Œçš„æŒ‡æ ‡å†å²æ•°æ®
    
    Returns:
        List of (step, value) tuples
    """
    try:
        history = client.get_metric_history(run_id, metric_name)
        return [(h.step, h.value) for h in history]
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•è·å–è¿è¡Œ {run_id[:8]} çš„æŒ‡æ ‡ {metric_name}: {e}")
        return []

def convert_steps_to_epochs(step_value_pairs: List[Tuple[int, float]], 
                           validation_frequency: int = 1) -> List[Tuple[int, float]]:
    """å°†stepè½¬æ¢ä¸ºepoch
    
    å‡è®¾validationåœ¨æ¯ä¸ªepochç»“æŸæ—¶æ‰§è¡Œ
    æ ¹æ®è§‚å¯Ÿåˆ°çš„æ•°æ®ï¼Œstep 62 å¯¹åº” epoch 0ï¼Œstep 125 å¯¹åº” epoch 1ï¼Œç­‰ç­‰
    è¿™æ„å‘³ç€validationæ­¥éª¤ä¹‹é—´å¤§çº¦ç›¸å·®63æ­¥
    """
    if not step_value_pairs:
        return []
    
    epoch_value_pairs = []
    
    # æ ¹æ®è§‚å¯Ÿåˆ°çš„æ•°æ®ï¼Œç¬¬ä¸€ä¸ªvalidation stepæ˜¯62ï¼Œæ¯ä¸ªepochå¤§çº¦å¢åŠ 63æ­¥
    # ä½†æ›´å‡†ç¡®çš„æ–¹æ³•æ˜¯ç›´æ¥è®¡ç®—ç›¸å¯¹epoch
    first_step = step_value_pairs[0][0]
    
    for i, (step, value) in enumerate(step_value_pairs):
        # ç®€åŒ–å‡è®¾ï¼šæ¯ä¸ªvalidation stepå¯¹åº”ä¸€ä¸ªepoch
        epoch = i
        epoch_value_pairs.append((epoch, value))
    
    return epoch_value_pairs

def align_epoch_data(all_generator_data: Dict[str, Dict[str, List[Tuple[int, float]]]]) -> Tuple[Dict, int]:
    """å¯¹é½ä¸åŒç”Ÿæˆå™¨çš„epochæ•°æ®
    
    Args:
        all_generator_data: {generator_type: {metric_name: [(epoch, value), ...]}}
    
    Returns:
        aligned_data: {generator_type: {metric_name: [value1, value2, ...]}}, max_epochs
    """
    # æ‰¾åˆ°æœ€å¤§epochæ•°
    max_epochs = 0
    for gen_data in all_generator_data.values():
        for metric_data in gen_data.values():
            if metric_data:
                max_epoch = max(epoch for epoch, _ in metric_data)
                max_epochs = max(max_epochs, max_epoch + 1)
    
    # å¯¹é½æ•°æ®
    aligned_data = {}
    for gen_type, gen_metrics in all_generator_data.items():
        aligned_data[gen_type] = {}
        for metric_name, epoch_values in gen_metrics.items():
            # åˆ›å»ºå®Œæ•´çš„epochæ•°ç»„ï¼Œç¼ºå¤±å€¼ç”¨NaNå¡«å……
            aligned_values = [np.nan] * max_epochs
            for epoch, value in epoch_values:
                if epoch < max_epochs:
                    aligned_values[epoch] = value
            aligned_data[gen_type][metric_name] = aligned_values
    
    return aligned_data, max_epochs

def collect_all_runs_data(experiment_names: List[str] = None) -> Dict[str, Dict[str, List[Tuple[int, float]]]]:
    """æ”¶é›†æ‰€æœ‰è¿è¡Œçš„å†å²æ•°æ®
    
    Returns:
        {generator_type: {metric_name: [(epoch, value), ...]}}
    """
    client = mlflow.tracking.MlflowClient()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå®éªŒåç§°ï¼Œè‡ªåŠ¨å‘ç°
    if experiment_names is None:
        experiments = client.search_experiments()
        experiment_names = [exp.name for exp in experiments 
                          if any(keyword in exp.name.lower() 
                                for keyword in ['gaussgan', 'classical', 'quantum'])]
    
    print(f"åˆ†æå®éªŒ: {experiment_names}")
    
    # å…³é”®æŒ‡æ ‡åˆ—è¡¨
    key_metrics = [
        'ValidationStep_FakeData_KLDivergence',
        'ValidationStep_FakeData_LogLikelihood', 
        'ValidationStep_FakeData_IsPositive',
        'ValidationStep_FakeData_WassersteinDistance',
        'ValidationStep_FakeData_MMDDistance',
        'train_g_loss_epoch',
        'd_loss',
        'g_loss'
    ]
    
    # æ”¶é›†æ•°æ®ï¼š{generator_type: {metric_name: [(epoch, value), ...]}}
    all_data = {}
    run_count = {}
    
    for exp_name in experiment_names:
        try:
            experiment = client.get_experiment_by_name(exp_name)
            if experiment is None:
                continue
                
            # è·å–æ‰€æœ‰è¿è¡Œï¼Œç„¶åè¿‡æ»¤
            all_runs = client.search_runs(
                experiment_ids=[experiment.experiment_id],
                order_by=["attribute.start_time desc"]
            )
            
            # åªä¿ç•™FINISHEDæˆ–RUNNINGçŠ¶æ€çš„è¿è¡Œ
            runs = [run for run in all_runs if run.info.status in ['FINISHED', 'RUNNING']]
            
            print(f"å®éªŒ '{exp_name}': æ‰¾åˆ° {len(runs)} ä¸ªå®Œæˆçš„è¿è¡Œ")
            
            for run in runs:
                generator_type = run.data.params.get('generator_type', 'unknown')
                
                if generator_type not in all_data:
                    all_data[generator_type] = {metric: [] for metric in key_metrics}
                    run_count[generator_type] = 0
                
                run_count[generator_type] += 1
                print(f"  å¤„ç†è¿è¡Œ {run.info.run_id[:8]}... (ç”Ÿæˆå™¨: {generator_type})")
                
                # æ”¶é›†æ¯ä¸ªæŒ‡æ ‡çš„å†å²æ•°æ®
                for metric in key_metrics:
                    if metric in run.data.metrics:
                        step_values = get_run_metric_history(client, run.info.run_id, metric)
                        if step_values:
                            epoch_values = convert_steps_to_epochs(step_values)
                            # å°†è¿™æ¬¡è¿è¡Œçš„æ•°æ®æ·»åŠ åˆ°æ€»æ•°æ®ä¸­
                            all_data[generator_type][metric].extend(epoch_values)
                
        except Exception as e:
            print(f"å¤„ç†å®éªŒ {exp_name} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"\næ•°æ®æ”¶é›†å®Œæˆ:")
    for gen_type, count in run_count.items():
        print(f"  {gen_type}: {count} æ¬¡è¿è¡Œ")
    
    return all_data

def aggregate_multiple_runs(data: Dict[str, Dict[str, List[Tuple[int, float]]]]) -> Dict[str, Dict[str, List[Tuple[int, float]]]]:
    """èšåˆåŒä¸€ç”Ÿæˆå™¨ç±»å‹çš„å¤šæ¬¡è¿è¡Œæ•°æ®
    
    å¯¹äºæ¯ä¸ª(generator_type, metric)ï¼Œè®¡ç®—æ¯ä¸ªepochçš„å¹³å‡å€¼
    """
    aggregated = {}
    
    for gen_type, metrics_data in data.items():
        aggregated[gen_type] = {}
        
        for metric_name, epoch_values in metrics_data.items():
            if not epoch_values:
                aggregated[gen_type][metric_name] = []
                continue
            
            # æŒ‰epochåˆ†ç»„
            epoch_groups = {}
            for epoch, value in epoch_values:
                if epoch not in epoch_groups:
                    epoch_groups[epoch] = []
                epoch_groups[epoch].append(value)
            
            # è®¡ç®—æ¯ä¸ªepochçš„å¹³å‡å€¼
            avg_data = []
            for epoch in sorted(epoch_groups.keys()):
                values = epoch_groups[epoch]
                # è¿‡æ»¤æ‰NaNå’Œæ— ç©·å€¼
                valid_values = [v for v in values if np.isfinite(v)]
                if valid_values:
                    avg_value = np.mean(valid_values)
                    avg_data.append((epoch, avg_value))
            
            aggregated[gen_type][metric_name] = avg_data
    
    return aggregated

def create_epoch_comparison_plots(aligned_data: Dict, max_epochs: int, output_dir: str = "docs"):
    """åˆ›å»ºé€epochå¯¹æ¯”å›¾è¡¨"""
    setup_chinese_fonts()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(output_dir).mkdir(exist_ok=True)
    
    # å…³é”®æŒ‡æ ‡é…ç½® - æ”¯æŒä¸­è‹±æ–‡æ ‡ç­¾
    if use_english_labels:
        metrics_config = {
            'ValidationStep_FakeData_KLDivergence': {
                'title': 'KL Divergence Trends (Lower is Better)',
                'ylabel': 'KL Divergence',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            },
            'ValidationStep_FakeData_LogLikelihood': {
                'title': 'Log Likelihood Trends (Higher is Better)',
                'ylabel': 'Log Likelihood',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            },
            'ValidationStep_FakeData_IsPositive': {
                'title': 'Positive Ratio Trends (Higher is Better)',
                'ylabel': 'Positive Ratio',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            },
            'ValidationStep_FakeData_WassersteinDistance': {
                'title': 'Wasserstein Distance Trends (Lower is Better)',
                'ylabel': 'Wasserstein Distance',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            },
            'ValidationStep_FakeData_MMDDistance': {
                'title': 'MMD Distance Trends (Lower is Better)',
                'ylabel': 'MMD Distance',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            },
            'train_g_loss_epoch': {
                'title': 'Generator Loss Trends',
                'ylabel': 'Generator Loss',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            }
        }
    else:
        metrics_config = {
            'ValidationStep_FakeData_KLDivergence': {
                'title': 'KLæ•£åº¦å˜åŒ–è¶‹åŠ¿ (è¶Šä½è¶Šå¥½)',
                'ylabel': 'KLæ•£åº¦',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            },
            'ValidationStep_FakeData_LogLikelihood': {
                'title': 'å¯¹æ•°ä¼¼ç„¶å˜åŒ–è¶‹åŠ¿ (è¶Šé«˜è¶Šå¥½)',
                'ylabel': 'å¯¹æ•°ä¼¼ç„¶',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            },
            'ValidationStep_FakeData_IsPositive': {
                'title': 'æ­£å€¼æ¯”ä¾‹å˜åŒ–è¶‹åŠ¿ (è¶Šé«˜è¶Šå¥½)',
                'ylabel': 'æ­£å€¼æ¯”ä¾‹',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            },
            'ValidationStep_FakeData_WassersteinDistance': {
                'title': 'Wassersteinè·ç¦»å˜åŒ–è¶‹åŠ¿ (è¶Šä½è¶Šå¥½)',
                'ylabel': 'Wassersteinè·ç¦»',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            },
            'ValidationStep_FakeData_MMDDistance': {
                'title': 'MMDè·ç¦»å˜åŒ–è¶‹åŠ¿ (è¶Šä½è¶Šå¥½)',
                'ylabel': 'MMDè·ç¦»',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            },
            'train_g_loss_epoch': {
                'title': 'ç”Ÿæˆå™¨æŸå¤±å˜åŒ–è¶‹åŠ¿',
                'ylabel': 'ç”Ÿæˆå™¨æŸå¤±',
                'color_map': {'classical_normal': 'blue', 'quantum_samples': 'red', 'quantum_shadows': 'green'}
            }
        }
    
    # åˆ›å»ºå­å›¾
    n_metrics = len(metrics_config)
    cols = 3
    rows = (n_metrics + cols - 1) // cols
    
    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
    if rows == 1:
        axes = axes.reshape(1, -1)
    elif cols == 1:
        axes = axes.reshape(-1, 1)
    
    epochs = list(range(max_epochs))
    
    for idx, (metric_name, config) in enumerate(metrics_config.items()):
        row = idx // cols
        col = idx % cols
        ax = axes[row][col]
        
        # ä¸ºæ¯ä¸ªç”Ÿæˆå™¨ç±»å‹ç»˜åˆ¶æ›²çº¿
        for gen_type, gen_data in aligned_data.items():
            if metric_name in gen_data:
                values = gen_data[metric_name]
                color = config['color_map'].get(gen_type, 'gray')
                
                # åªç»˜åˆ¶æœ‰æ•°æ®çš„éƒ¨åˆ†
                valid_epochs = []
                valid_values = []
                for i, val in enumerate(values):
                    if np.isfinite(val):
                        valid_epochs.append(i)
                        valid_values.append(val)
                
                if valid_epochs:
                    ax.plot(valid_epochs, valid_values, 
                           label=f'{gen_type}', 
                           color=color, 
                           marker='o', 
                           markersize=4,
                           linewidth=2,
                           alpha=0.8)
        
        ax.set_title(config['title'], fontsize=12, fontweight='bold')
        ax.set_xlabel('Epoch', fontsize=10)
        ax.set_ylabel(config['ylabel'], fontsize=10)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # å¦‚æœæœ‰æ•°æ®ï¼Œè®¾ç½®åˆç†çš„yè½´èŒƒå›´
        all_values = []
        for gen_data in aligned_data.values():
            if metric_name in gen_data:
                valid_vals = [v for v in gen_data[metric_name] if np.isfinite(v)]
                all_values.extend(valid_vals)
        
        if all_values and len(all_values) > 0:
            y_min, y_max = min(all_values), max(all_values)
            if np.isfinite(y_min) and np.isfinite(y_max) and y_min != y_max:
                y_range = y_max - y_min
                if y_range > 0:
                    ax.set_ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(len(metrics_config), rows * cols):
        row = idx // cols
        col = idx % cols
        axes[row][col].set_visible(False)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    output_path = Path(output_dir) / "epoch_comparison_plots.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    print(f"é€epochå¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    return str(output_path)

def save_detailed_csv(aligned_data: Dict, max_epochs: int, output_dir: str = "docs"):
    """ä¿å­˜è¯¦ç»†çš„CSVæ•°æ®æ–‡ä»¶"""
    Path(output_dir).mkdir(exist_ok=True)
    
    # åˆ›å»ºå®Œæ•´çš„DataFrame
    data_rows = []
    
    for epoch in range(max_epochs):
        for gen_type, gen_data in aligned_data.items():
            row = {'epoch': epoch, 'generator_type': gen_type}
            
            for metric_name, values in gen_data.items():
                if epoch < len(values):
                    row[metric_name] = values[epoch]
                else:
                    row[metric_name] = np.nan
            
            data_rows.append(row)
    
    df = pd.DataFrame(data_rows)
    
    # ä¿å­˜CSV
    csv_path = Path(output_dir) / "epoch_comparison_detailed.csv"
    df.to_csv(csv_path, index=False)
    
    print(f"è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {csv_path}")
    
    # åŒæ—¶ä¿å­˜æ±‡æ€»ç»Ÿè®¡
    summary_data = []
    for gen_type, gen_data in aligned_data.items():
        summary_row = {'generator_type': gen_type}
        
        for metric_name, values in gen_data.items():
            valid_values = [v for v in values if np.isfinite(v)]
            if valid_values:
                summary_row.update({
                    f'{metric_name}_final': valid_values[-1],
                    f'{metric_name}_best': min(valid_values) if 'Distance' in metric_name or 'KL' in metric_name else max(valid_values),
                    f'{metric_name}_mean': np.mean(valid_values),
                    f'{metric_name}_std': np.std(valid_values),
                    f'{metric_name}_epochs_count': len(valid_values)
                })
        
        summary_data.append(summary_row)
    
    summary_df = pd.DataFrame(summary_data)
    summary_path = Path(output_dir) / "epoch_comparison_summary.csv"
    summary_df.to_csv(summary_path, index=False)
    
    print(f"æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜åˆ°: {summary_path}")
    
    return str(csv_path), str(summary_path)

def print_convergence_analysis(aligned_data: Dict, max_epochs: int):
    """æ‰“å°æ”¶æ•›åˆ†æç»“æœ"""
    print("\n" + "="*80)
    print("é€Epochæ”¶æ•›åˆ†æ")
    print("="*80)
    
    for gen_type, gen_data in aligned_data.items():
        print(f"\nğŸ“Š {gen_type} ç”Ÿæˆå™¨åˆ†æ:")
        print("-" * 50)
        
        # åˆ†æKLæ•£åº¦æ”¶æ•›
        if 'ValidationStep_FakeData_KLDivergence' in gen_data:
            kl_values = gen_data['ValidationStep_FakeData_KLDivergence']
            valid_kl = [v for v in kl_values if np.isfinite(v)]
            
            if len(valid_kl) > 1:
                initial_kl = valid_kl[0]
                final_kl = valid_kl[-1]
                best_kl = min(valid_kl)
                best_epoch = next(i for i, v in enumerate(kl_values) if v == best_kl)
                
                improvement = ((initial_kl - final_kl) / abs(initial_kl) * 100) if initial_kl != 0 else 0
                
                print(f"  KLæ•£åº¦: {initial_kl:.4f} -> {final_kl:.4f} (æ”¹è¿› {improvement:+.1f}%)")
                print(f"  æœ€ä½³KLæ•£åº¦: {best_kl:.4f} (åœ¨ç¬¬ {best_epoch} epoch)")
                print(f"  è®­ç»ƒepochs: {len(valid_kl)}")
                
                # è®¡ç®—æ”¶æ•›ç¨³å®šæ€§ï¼ˆæœ€å5ä¸ªepochçš„æ ‡å‡†å·®ï¼‰
                if len(valid_kl) >= 5:
                    stability = np.std(valid_kl[-5:])
                    print(f"  æœ€ç»ˆç¨³å®šæ€§: {stability:.4f} (æœ€å5ä¸ªepochçš„æ ‡å‡†å·®)")
        
        # åˆ†ææ­£å€¼æ¯”ä¾‹
        if 'ValidationStep_FakeData_IsPositive' in gen_data:
            pos_values = gen_data['ValidationStep_FakeData_IsPositive']
            valid_pos = [v for v in pos_values if np.isfinite(v)]
            
            if valid_pos:
                final_pos = valid_pos[-1]
                best_pos = max(valid_pos)
                print(f"  æ­£å€¼æ¯”ä¾‹: æœ€ç»ˆ {final_pos:.3f}, æœ€ä½³ {best_pos:.3f}")

def compare_generators_epoch_by_epoch(experiment_names: List[str] = None, output_dir: str = "docs"):
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œé€epochå¯¹æ¯”åˆ†æ"""
    setup_chinese_fonts()
    
    print("=" * 80)
    print("é‡å­vså¤å…¸ç”Ÿæˆå™¨é€Epochæ€§èƒ½å¯¹æ¯”åˆ†æ")
    print("=" * 80)
    
    # æ”¶é›†æ‰€æœ‰è¿è¡Œçš„å†å²æ•°æ®
    print("\næ­¥éª¤1: æ”¶é›†å†å²æ•°æ®...")
    all_runs_data = collect_all_runs_data(experiment_names)
    
    if not all_runs_data:
        print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å®éªŒæ•°æ®")
        return None
    
    # èšåˆå¤šæ¬¡è¿è¡Œçš„æ•°æ®
    print("\næ­¥éª¤2: èšåˆå¤šæ¬¡è¿è¡Œæ•°æ®...")
    aggregated_data = aggregate_multiple_runs(all_runs_data)
    
    # å¯¹é½epochæ•°æ®
    print("\næ­¥éª¤3: å¯¹é½epochæ•°æ®...")
    aligned_data, max_epochs = align_epoch_data(aggregated_data)
    
    print(f"æœ€å¤§è®­ç»ƒepochs: {max_epochs}")
    
    # åˆ›å»ºå¯è§†åŒ–
    print("\næ­¥éª¤4: åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    plot_path = create_epoch_comparison_plots(aligned_data, max_epochs, output_dir)
    
    # ä¿å­˜è¯¦ç»†æ•°æ®
    print("\næ­¥éª¤5: ä¿å­˜è¯¦ç»†æ•°æ®...")
    csv_path, summary_path = save_detailed_csv(aligned_data, max_epochs, output_dir)
    
    # æ‰“å°åˆ†æç»“æœ
    print_convergence_analysis(aligned_data, max_epochs)
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*80)
    print("æœ€ç»ˆå¯¹æ¯”æŠ¥å‘Š")
    print("="*80)
    
    for gen_type, gen_data in aligned_data.items():
        print(f"\nğŸ”¹ {gen_type}:")
        
        # æ‰¾åˆ°æœ‰æ•ˆçš„æŒ‡æ ‡
        for metric_name, values in gen_data.items():
            valid_values = [v for v in values if np.isfinite(v)]
            if valid_values and len(valid_values) > 0:
                if 'KL' in metric_name:
                    print(f"   KLæ•£åº¦: åˆå§‹ {valid_values[0]:.4f} -> æœ€ç»ˆ {valid_values[-1]:.4f}")
                elif 'LogLikelihood' in metric_name:
                    print(f"   å¯¹æ•°ä¼¼ç„¶: åˆå§‹ {valid_values[0]:.4f} -> æœ€ç»ˆ {valid_values[-1]:.4f}")
                elif 'IsPositive' in metric_name:
                    print(f"   æ­£å€¼æ¯”ä¾‹: åˆå§‹ {valid_values[0]:.3f} -> æœ€ç»ˆ {valid_values[-1]:.3f}")
                break  # åªæ˜¾ç¤ºä¸€ä¸ªä¸»è¦æŒ‡æ ‡é¿å…é‡å¤
    
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   - è¯¦ç»†å›¾è¡¨: {plot_path}")
    print(f"   - è¯¦ç»†æ•°æ®: {csv_path}")
    print(f"   - æ±‡æ€»ç»Ÿè®¡: {summary_path}")
    
    return {
        'aligned_data': aligned_data,
        'max_epochs': max_epochs,
        'output_files': {
            'plot': plot_path,
            'detailed_csv': csv_path,
            'summary_csv': summary_path
        }
    }

def quick_status_check():
    """å¿«é€Ÿæ£€æŸ¥å¯ç”¨çš„å®éªŒå’Œè¿è¡ŒçŠ¶æ€"""
    client = mlflow.tracking.MlflowClient()
    experiments = client.search_experiments()
    
    print("å¯ç”¨å®éªŒçŠ¶æ€:")
    print("-" * 60)
    
    total_finished = 0
    total_running = 0
    total_failed = 0
    
    for exp in experiments:
        if any(keyword in exp.name.lower() for keyword in ['gaussgan', 'classical', 'quantum']):
            runs = client.search_runs([exp.experiment_id])
            finished = len([r for r in runs if r.info.status == 'FINISHED'])
            running = len([r for r in runs if r.info.status == 'RUNNING'])
            failed = len([r for r in runs if r.info.status == 'FAILED'])
            
            total_finished += finished
            total_running += running
            total_failed += failed
            
            print(f"{exp.name[:40]:<40}: å®Œæˆ {finished}, è¿è¡Œä¸­ {running}, å¤±è´¥ {failed}")
    
    print("-" * 60)
    print(f"æ€»è®¡: å®Œæˆ {total_finished}, è¿è¡Œä¸­ {total_running}, å¤±è´¥ {total_failed}")
    print()

if __name__ == "__main__":
    # æ£€æŸ¥å½“å‰çŠ¶æ€
    quick_status_check()
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    try:
        results = compare_generators_epoch_by_epoch()
        
        if results:
            print("\nâœ… åˆ†æå®Œæˆï¼")
            print("\nå…³é”®æ´å¯Ÿ:")
            print("- å¯ä»¥çœ‹åˆ°æ¯ä¸ªç”Ÿæˆå™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€§èƒ½å˜åŒ–")
            print("- ä¸åŒepochæ•°çš„ç”Ÿæˆå™¨ä¼šç”¨NaNå¡«å……å¯¹é½")
            print("- æ‰€æœ‰å›¾è¡¨éƒ½æ”¯æŒä¸­æ–‡æ˜¾ç¤º")
            print("- è¯¦ç»†æ•°æ®å¯ç”¨äºè¿›ä¸€æ­¥åˆ†æ")
        else:
            print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥MLflowæ•°æ®")
            
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()